{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from carn_utils.carn_dataloader import copy_by_fold\n",
        "from carn_utils.split_utils import carn_split_keys, export_split_tsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the function with your parameters\n",
        "dataset_path = '../../datasets'\n",
        "output_dir = 'data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carnatic Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#copy_by_fold(dataset_path=dataset_path, validate=False, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "seed = 0\n",
        "folds = [1,2]\n",
        "\n",
        "for fold in folds:\n",
        "    print(f\"Processing fold {fold}...\")\n",
        "    # Get the split keys for the current fold\n",
        "    export_split_tsv(dataset_path=dataset_path, train_fold=fold, seed=seed, csv_path='cmr_splits.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!python launch_scripts/preprocess_audio.py --orig_audio_paths data/audio_paths.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finetune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Disabling MPS fallback for PyTorch.\n",
            "Seed set to 0\n",
            "Starting a new run with the following parameters:\n",
            "Namespace(name='carnatic-finetune', gpu=0, force_flash_attention=False, compile=['frontend', 'transformer_blocks', 'task_heads'], n_layers=6, transformer_dim=512, frontend_dropout=0.1, transformer_dropout=0.2, lr=0.0008, weight_decay=0.01, logger='none', num_workers=8, n_heads=16, fps=50, loss='shift_tolerant_weighted_bce', warmup_steps=1000, max_epochs=100, batch_size=8, accumulate_grad_batches=8, train_length=1500, dbn=False, eval_trim_beats=5, val_frequency=5, tempo_augmentation=True, pitch_augmentation=True, mask_augmentation=True, sum_head=True, partial_transformers=True, length_based_oversampling_factor=0.65, val=True, hung_data=False, fold=None, seed=0, checkpoint='final0', annotation_dir='cmr-fold1', use_cpu=True)\n",
            "Using data directory: /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/navid/beat_this/data\n",
            "Using checkpoint directory: /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/navid/beat_this/checkpoints\n",
            "Validation set: 18 items from: cmr-fold1\n",
            "Training set oversampled from 70 to 513 excerpts.\n",
            "Training set: 513 items from: cmr-fold1\n",
            "Using positive weights:  {'beat': 17, 'downbeat': 119}\n",
            "Will compile model frontend\n",
            "Will compile model transformer_blocks\n",
            "Will compile model task_heads\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beatnavid/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (mps), used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beatnavid/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beatnavid/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "Loading model from checkpoint: /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/navid/beat_this/checkpoints/final0.ckpt\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beatnavid/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/navid/beat_this/checkpoints exists and is not empty.\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beatnavid/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beatnavid/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "\n",
            "  | Name          | Type                 | Params | Mode \n",
            "---------------------------------------------------------------\n",
            "0 | model         | BeatThis             | 20.3 M | train\n",
            "1 | beat_loss     | ShiftTolerantBCELoss | 0      | train\n",
            "2 | downbeat_loss | ShiftTolerantBCELoss | 0      | train\n",
            "---------------------------------------------------------------\n",
            "20.3 M    Trainable params\n",
            "16        Non-trainable params\n",
            "20.3 M    Total params\n",
            "81.007    Total estimated model params size (MB)\n",
            "233       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]/Users/satyajeetprabhu/miniconda3/envs/beatnavid/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
            "Epoch 0:   0%|                                           | 0/64 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "!python launch_scripts/train.py --name carnatic-finetune --checkpoint final0 --annotation-dir cmr-fold1 --gpu 0 --use-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed set to 0\n",
            "Starting a new run with the following parameters:\n",
            "Namespace(name='final0_ft', gpu=0, force_flash_attention=False, compile=['frontend', 'transformer_blocks', 'task_heads'], n_layers=6, transformer_dim=512, frontend_dropout=0.1, transformer_dropout=0.2, lr=0.0008, weight_decay=0.01, logger='none', num_workers=8, n_heads=16, fps=50, loss='shift_tolerant_weighted_bce', warmup_steps=1000, max_epochs=50, batch_size=8, accumulate_grad_batches=8, train_length=1500, dbn=False, eval_trim_beats=5, val_frequency=5, tempo_augmentation=True, pitch_augmentation=True, mask_augmentation=True, sum_head=True, partial_transformers=True, length_based_oversampling_factor=0.65, val=True, hung_data=False, fold=None, seed=0, load_from_checkpoint='final0', use_cpu=True)\n",
            "Using data directory: /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/beat_this_carnatic/data\n",
            "Using checkpoint directory: /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/beat_this_carnatic/checkpoints\n",
            "Validation set: 36 items from: cmr-fold1 cmr-fold2\n",
            "Training set oversampled from 140 to 1046 excerpts.\n",
            "Training set: 1046 items from: cmr-fold1 cmr-fold2\n",
            "Using positive weights:  {'beat': 17, 'downbeat': 123}\n",
            "Loading model from checkpoint: /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/beat_this_carnatic/checkpoints/final0.ckpt\n",
            "Will compile model frontend\n",
            "Will compile model transformer_blocks\n",
            "Will compile model task_heads\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beathis/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (mps), used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beathis/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beathis/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beathis/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory /Users/satyajeetprabhu/Desktop/Projects/Meter Tracking/beat_this_carnatic/checkpoints exists and is not empty.\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beathis/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
            "/Users/satyajeetprabhu/miniconda3/envs/beathis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "\n",
            "  | Name          | Type                 | Params | Mode \n",
            "---------------------------------------------------------------\n",
            "0 | model         | BeatThis             | 20.3 M | train\n",
            "1 | beat_loss     | ShiftTolerantBCELoss | 0      | train\n",
            "2 | downbeat_loss | ShiftTolerantBCELoss | 0      | train\n",
            "---------------------------------------------------------------\n",
            "20.3 M    Trainable params\n",
            "16        Non-trainable params\n",
            "20.3 M    Total params\n",
            "81.007    Total estimated model params size (MB)\n",
            "233       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]/Users/satyajeetprabhu/miniconda3/envs/beathis/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
            "Epoch 0:   0%|                                          | 0/130 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "#!python launch_scripts/train.py --name test --checkpoint final0.ckpt --annotation-dir cmr-fold1\n",
        "#!python launch_scripts/train.py --load-from-checkpoint final0 --name final0_ft --max-epochs 50 --use-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train_fold = \"cmr-fold1\"  # <-- change this as needed\n",
        "\n",
        "# Dictionary mapping fold names to their audio folder paths\n",
        "available_folds = {\n",
        "    \"cmr-fold1\": \"data/audio/cmr-fold1\",\n",
        "    \"cmr-fold2\": \"data/audio/cmr-fold2\",\n",
        "    # Add more folds here as needed\n",
        "}\n",
        "# Build the TSV entry\n",
        "fold_audio_paths = [[train_fold, available_folds[train_fold]]]\n",
        "audio_paths_df = pd.DataFrame(fold_audio_paths, columns=[\"fold_name\", \"audio_path\"])\n",
        "tsv_path = os.path.join(output_dir, \"audio_paths.tsv\")\n",
        "# Write TSV with tab separator, no header or index\n",
        "audio_paths_df.to_csv(tsv_path, sep=\"\\t\", index=False, header=False)\n",
        "\n",
        "print(f\"âœ… Created TSV file for '{train_fold}' at: {tsv_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOW4OkTmphTrvw2IQLr+kxP",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "beatnavid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
